{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from sketch.examples.prompt_machine import *\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "import random\n",
    "\n",
    "PM_SETTINGS[\"VERBOSE\"] = False\n",
    "SPIDER_DB_PATH = \"/home/jawaugh/benchmarks/raw/spider/spider/database/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_file_from_google_drive(id, destination):\n",
    "#     URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "#     session = requests.Session()\n",
    "\n",
    "#     response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "#     token = get_confirm_token(response)\n",
    "\n",
    "#     if token:\n",
    "#         params = { 'id' : id, 'confirm' : token }\n",
    "#         response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "#     save_response_content(response, destination)    \n",
    "\n",
    "# def get_confirm_token(response):\n",
    "#     for key, value in response.cookies.items():\n",
    "#         if key.startswith('download_warning'):\n",
    "#             return value\n",
    "\n",
    "#     return None\n",
    "\n",
    "# def save_response_content(response, destination):\n",
    "#     CHUNK_SIZE = 32768\n",
    "\n",
    "#     with open(destination, \"wb\") as f:\n",
    "#         for chunk in response.iter_content(CHUNK_SIZE):\n",
    "#             if chunk: # filter out keep-alive new chunks\n",
    "#                 f.write(chunk)\n",
    "\n",
    "# download_file_from_google_drive(\"1m_M2VOjM7Xxq0z9hxgGdYIGIKhxOIU-F\", \"spider_train.json\")\n",
    "# download_file_from_google_drive(\"1twy32bdOYcTY8HXrISm1vTBMMjTzI9Fa\", \"spider_eval.json\")\n",
    "\n",
    "data = json.load(open(\"spider_train.json\"))\n",
    "for i, row in enumerate(data):\n",
    "    row['row_id'] = f\"spider_train|{i}\"\n",
    "\n",
    "data_eval = json.load(open(\"spider_eval.json\"))\n",
    "for i, row in enumerate(data):\n",
    "    row['row_id'] = f\"spider_eval|{i}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare executed results\n",
    "\n",
    "def get_result(db_name, sql):\n",
    "    conn = sqlite3.connect(os.path.join(SPIDER_DB_PATH, f\"{db_name}/{db_name}.sqlite\"))\n",
    "    conn.text_factory = lambda b: b.decode(errors=\"ignore\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    return cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_database_context(db_name):\n",
    "    db_path = os.path.join(SPIDER_DB_PATH, f\"{db_name}/{db_name}.sqlite\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    # execute a sql query to get all the tables and the columns in the tables\n",
    "    c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    outputschema = \"\"\n",
    "    # outputschema = \"Example: table.column* (row1 value) *means join key\\n\"\n",
    "    for (table,) in c.fetchall():\n",
    "        # # # STYLE 2\n",
    "        # outputschema += f\"===== {table} =====\\n\"\n",
    "        # c.execute(f\"PRAGMA table_info({table})\")\n",
    "        # column_names = [column for _, column, dtype, *_ in c.fetchall()]\n",
    "        # output = c.execute(f\"select * from {table} limit 2\").fetchall()\n",
    "        # for i, column in enumerate(column_names):\n",
    "        #     if len(output) == 0:\n",
    "        #         outputschema += f\"{column} : -empty-\"\n",
    "        #     elif len(output) == 1:\n",
    "        #         outputschema += f\"{column} : {output[0][i]}\"\n",
    "        #     else:\n",
    "        #         outputschema += f\"{column} : {output[0][i]} | {output[1][i]}\"\n",
    "        #     outputschema += \"\\n\"\n",
    "\n",
    "        # # STYLE 1\n",
    "        outputschema += f\"{table} (\"\n",
    "        fks = [(tab, fro, to) for _, _, tab, fro, to, *_ in c.execute(f\"PRAGMA foreign_key_list({table})\").fetchall()]\n",
    "        cols_to_star = [co for _, co, _ in fks]\n",
    "        c.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns = [column + (\"*\" if pk or column in cols_to_star else \"\") for _, column, dtype, _, _, pk in c.fetchall() ]\n",
    "        outputschema += \"|\".join(columns)\n",
    "        outputschema += \") \\n\"\n",
    "        output = c.execute(f\"select * from {table} limit 2\").fetchall()\n",
    "        if len(output) == 0:\n",
    "            outputschema += \"Empty Table\"\n",
    "        else:\n",
    "            for row in output:\n",
    "                outputschema += \"|\".join([str(x) for x in row]) + \"\\n\"\n",
    "        outputschema += \"  foreign keys: \" + \", \".join([f\"{fro} -> {to} ({tab})\" for tab, fro, to in fks]) + \"\\n\"\n",
    "        outputschema += \"\\n\"\n",
    "\n",
    "        # # STYLE 3\n",
    "        # fks = [(tab, fro, to) for _, _, tab, fro, to, *_ in c.execute(f\"PRAGMA foreign_key_list({table})\").fetchall()]\n",
    "        # cols_to_star = [co for _, co, _ in fks]\n",
    "        # columns = [column + (\"*\" if pk or column in cols_to_star else \"\") for _, column, dtype, _, _, pk in c.execute(f\"PRAGMA table_info({table})\").fetchall() ]\n",
    "        # output = c.execute(f\"select * from {table} limit 2\").fetchall()\n",
    "        # outputschema += f\"== {table} ==\\n\"\n",
    "        # for i, column in enumerate(columns):\n",
    "        #     if len(output) == 0:\n",
    "        #         outputschema += f\"{column} ()\"\n",
    "        #     elif len(output) == 1:\n",
    "        #         outputschema += f\"{column} ({output[0][i]})\"\n",
    "        #     else:\n",
    "        #         outputschema += f\"{column} ({output[0][i]}|{output[1][i]})\"\n",
    "        #     outputschema += \"\\n\"\n",
    "        # outputschema += \"\\n\"\n",
    "    return outputschema\n",
    "\n",
    "get_db_context = asyncPrompt(\"get_database_context\", get_database_context)\n",
    "correct_sqlite_error = asyncGPT3Edit(\n",
    "    \"correct_sqlite_error\",\n",
    "    \"Update the query to fix the error (correct the sql query between the ``` marks): [{{ error }}]\",\n",
    "    model_name=\"code-davinci-edit-001\",\n",
    "    temperature=0.0,\n",
    "    )\n",
    "\n",
    "# gpt3_zeroshot_sql = asyncGPT3Prompt(\"gpt3_zeroshot_sql\", \"\"\"\n",
    "# Database Context\n",
    "# {{ db_context }}\n",
    "# SQLite Query for question [{{ question }}]:\n",
    "# ```\n",
    "# \"\"\", stop=\"```\", temperature=0.0)\n",
    "\n",
    "# gpt3_zeroshot_explain_plan = asyncGPT3Prompt(\"gpt3_zeroshot_explain_plan\", \"\"\"\n",
    "# Database Context\n",
    "# {{ db_context }}\n",
    "# SQLite Query for question [{{ question }}]...\n",
    "# 1) First we need to consider\n",
    "# \"\"\", stop=\"```\", temperature=1.0, model_name=\"text-davinci-002\")\n",
    "\n",
    "gpt3_zeroshot_sql_warm = asyncGPT3Prompt(\"gpt3_zeroshot_sql_warm\", \"\"\"\n",
    "Database Context\n",
    "{{ db_context }}\n",
    "SQLite Query for question [{{ question }}]:\n",
    "```\"\"\", stop=\"```\", temperature=0.4, model_name=\"code-davinci-002\")\n",
    "\n",
    "def clean_sql_quick(sql):\n",
    "    sql = sql.strip()\n",
    "    sql = sql.replace(\"\\n\", \"\")\n",
    "    removals = [\"sqlite3\", \"sqlite\", \"sql\", \"SQL\"]\n",
    "    for removal in sorted(removals, key=lambda x: len(x), reverse=True):\n",
    "        if sql.startswith(removal):\n",
    "            sql = sql[len(removal):]\n",
    "    return sql\n",
    "\n",
    "async def get_sql_for_question_multi(db_name, text_in, top_n=5):\n",
    "    db_context = await get_db_context(db_name)\n",
    "    sqls = await asyncio.gather(*[gpt3_zeroshot_sql_warm(db_context=db_context, question=text_in) for _ in range(top_n)])\n",
    "    sqls = list(set([clean_sql_quick(sql) for sql in sqls]))\n",
    "\n",
    "    async def run_sql_for_result(sql):\n",
    "        trials = 0\n",
    "        while trials < 4:\n",
    "            try:\n",
    "                return (sql, get_result(db_name, sql))\n",
    "            except Exception as e:\n",
    "                last_prompt = gpt3_zeroshot_sql_warm.get_prompt(db_context=db_context, question=text_in) + sql + '```'\n",
    "                response = await correct_sqlite_error(error=str(e), input=last_prompt)\n",
    "                try:\n",
    "                    sql = response.split('```')[1]\n",
    "                    sql = clean_sql_quick(sql)\n",
    "                except:\n",
    "                    sql = \"\"\n",
    "                trials += 1\n",
    "        return (sql, None)\n",
    "        \n",
    "    results = await asyncio.gather(*[run_sql_for_result(sql) for sql in sqls])\n",
    "\n",
    "    answers = {json.dumps(res): set() for _, res in results if res is not None}\n",
    "    for sql, res in results:\n",
    "        if res is not None and sql:\n",
    "            answers[json.dumps(res)].add(sql)\n",
    "    if len(answers) == 0:\n",
    "        return \"FAILED TO GENERATE SQL\"\n",
    "\n",
    "    top_answers = sorted(answers.values(), key=len, reverse=True)[0]\n",
    "    return next(iter(top_answers))\n",
    "\n",
    "get_sql_from_text_multi = asyncPrompt(\"get_sql_from_text_multi\", get_sql_for_question_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def row_runner(row, verbose=False):\n",
    "    sql_answers = {\n",
    "        'query_answer': row['query'],\n",
    "        'gpt3_answer': await get_sql_from_text_multi(row['db_id'], row['text_in']),\n",
    "    }\n",
    "    sql_executed = {}\n",
    "    for k, query in sql_answers.items():\n",
    "        newrow = k.split('_')[0]+'_data'\n",
    "        try:\n",
    "            sql_executed[k.split('_')[0]+'_data'] = get_result(row['db_id'], query)\n",
    "        except Exception as e:\n",
    "            sql_executed[k.split('_')[0]+'_data'] = str(e)\n",
    "\n",
    "    if not isinstance(sql_executed['query_data'], list):\n",
    "        print(\"Gold failed to execute\")\n",
    "\n",
    "    execution_equals = {}\n",
    "    for k, v in sql_executed.items():\n",
    "        newrow = k.replace('data', 'is_equal')\n",
    "        if not isinstance(v, list):\n",
    "            execution_equals[newrow] = False\n",
    "        execution_equals[newrow] = v == sql_executed['query_data']\n",
    "\n",
    "    execution_is_superset = {}\n",
    "    for k, v in sql_executed.items():\n",
    "        newrow = k.replace('data', 'is_superset')\n",
    "        if not isinstance(v, list):\n",
    "            execution_is_superset[newrow] = False\n",
    "        if len(v) != len(sql_executed['query_data']):\n",
    "            execution_is_superset[newrow] = False\n",
    "        else:\n",
    "            execution_is_superset[newrow] = all([set(x) >= set(y) for (x, y) in zip(v, sql_executed['query_data'])])\n",
    "    \n",
    "    execution_is_same_set_of_rows = {}\n",
    "    for k, v in sql_executed.items():\n",
    "        newrow = k.replace('data', 'is_same_set_of_rows')\n",
    "        if not isinstance(v, list):\n",
    "            execution_is_same_set_of_rows[newrow] = False\n",
    "        execution_is_same_set_of_rows[newrow] = set(v) == set(sql_executed['query_data'])\n",
    "    return sql_answers | sql_executed | execution_equals | execution_is_superset | execution_is_same_set_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await row_runner(data[1101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(i, x) for i, x in enumerate(data) if x['question'] == 'What are the first and last names of the customers with the 10 cheapest invoices?']\n",
    "# thing = random.sample(data, 1)[0]\n",
    "# rowid = random.randint(0, len(data))\n",
    "# thing = data[rowid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(thing['question'])\n",
    "# print(thing['db_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "results = []\n",
    "for i in random.sample(range(len(data)), 400):\n",
    "    thing = data[i]\n",
    "    print(i, thing['question'], thing['db_id'])\n",
    "    result = await row_runner(thing)\n",
    "    result |= {'row_number': i, 'question': thing['question'], 'db_id': thing['db_id']}\n",
    "    results.append(result)\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df['query_is_equal'].sum())\n",
    "    print(df[['gpt3_is_equal', 'gpt3_is_superset', 'gpt3_is_same_set_of_rows']].sum())\n",
    "    time.sleep(10.0)\n",
    "# import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_prompttext_for_id(row_id):\n",
    "    db_name, text_in = data[row_id]['db_id'], data[row_id]['question']\n",
    "    db_context = await get_db_context(db_name)\n",
    "    return gpt3_zeroshot_sql_warm.get_prompt(db_context=db_context, question=text_in)\n",
    "\n",
    "print(await get_prompttext_for_id(3903))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_result(\"movie_1\", \"SELECT distinct stars from rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_result(\"game_injury\", \"select * from game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_result(\"hr_1\", \"select employees.department_id, commission_pct, department_name, count(*) from employees join departments on employees.department_id = departments.department_id  group by employees.department_id, commission_pct \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_result('products_gen_characteristics', \"SELECT product_name FROM Products WHERE color_code != (SELECT color_code FROM Ref_Colors WHERE color_description = 'white') AND product_category_code NOT IN (SELECT product_category_code FROM Ref_Product_Categories WHERE unit_of_measure = 'Handful');\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def row_cosql_runner(row, their_answer, verbose=False):\n",
    "    sql_answers = {\n",
    "        'query_answer': row['query'],\n",
    "        'unifiedskg_answer': their_answer,\n",
    "    }\n",
    "    sql_executed = {}\n",
    "    for k, query in sql_answers.items():\n",
    "        newrow = k.split('_')[0]+'_data'\n",
    "        try:\n",
    "            sql_executed[k.split('_')[0]+'_data'] = get_result(row['db_id'], query)\n",
    "        except Exception as e:\n",
    "            sql_executed[k.split('_')[0]+'_data'] = str(e)\n",
    "\n",
    "    if not isinstance(sql_executed['query_data'], list):\n",
    "        print(\"Gold failed to execute\")\n",
    "\n",
    "    execution_equals = {}\n",
    "    for k, v in sql_executed.items():\n",
    "        newrow = k.replace('data', 'is_equal')\n",
    "        if not isinstance(v, list):\n",
    "            execution_equals[newrow] = False\n",
    "        execution_equals[newrow] = v == sql_executed['query_data']\n",
    "\n",
    "    execution_is_superset = {}\n",
    "    for k, v in sql_executed.items():\n",
    "        newrow = k.replace('data', 'is_superset')\n",
    "        if not isinstance(v, list):\n",
    "            execution_is_superset[newrow] = False\n",
    "        if len(v) != len(sql_executed['query_data']):\n",
    "            execution_is_superset[newrow] = False\n",
    "        else:\n",
    "            execution_is_superset[newrow] = all([set(x) >= set(y) for (x, y) in zip(v, sql_executed['query_data'])])\n",
    "    \n",
    "    execution_is_same_set_of_rows = {}\n",
    "    for k, v in sql_executed.items():\n",
    "        newrow = k.replace('data', 'is_same_set_of_rows')\n",
    "        if not isinstance(v, list):\n",
    "            execution_is_same_set_of_rows[newrow] = False\n",
    "        execution_is_same_set_of_rows[newrow] = set(v) == set(sql_executed['query_data'])\n",
    "    return sql_answers | sql_executed | execution_equals | execution_is_superset | execution_is_same_set_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "results = []\n",
    "for i in random.sample(range(len(data_eval)), 400):\n",
    "    thing = data_eval[i]\n",
    "    print(i, thing['question'], thing['db_id'])\n",
    "    result = await row_cosql_runner(thing)\n",
    "    result |= {'row_number': i, 'question': thing['question'], 'db_id': thing['db_id']}\n",
    "    results.append(result)\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df['query_is_equal'].sum())\n",
    "    # print(df[['seq_out_is_equal', 'gpt3_is_superset', 'gpt3_is_same_set_of_rows']].sum())\n",
    "# import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(row):\n",
    "    return \"{}; structed knowledge: {}\".format(row['text_in'], row['struct_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrongs = [3237,1515,2889,5829,1739,1084,2582,267,1373,6578,3690,1372,2761,2375,6677,5306,593,6802,1735,3315]\n",
    "# batch = [1223,4560,2079,4177,1819,302,1018,987,6894,2204,537,4071,191,6732,1604,1966,3620,2591,3237,1716,6848,5590,6731,2667,5040,5551,2722,5961,3714,4541,369,1620,1310,1829,3055,5657,4804,3873,4198,5174,5820,2331,3812,3726,3982,4310,4239,4289,1515,468,4356,1646,3390,1191,2250,3710,5689,2914,3853,2065,1981,3912,183,5222,4278,2889,5357,3981,4562,587,823,2187,5829,6113,795,2199,1739,5540,5371,1084,3505,1176,4536,1291,3813,2165,1341,2582,3112,3168,1974,6625,3340,267,1373,4316,5291,425,6253,780,2775,4075,4553,5480,3655,4602,6361,156,636,2674,1967,5544,5231,1370,4376,4058,1567,6394,5074,1399,4155,1225,6578,1398,1194,820,3727,5525,2691,3903,4945,3607,5759,6900,6127,3407,2875,6908,4577,5957,6623,3690,2288,5564,4985,2948,4422,5882,1103,6268,1056,2782,3308,576,6850,5121,4543,1771,6416,3859,5335,2169,6231,4041,4865,4665,338,1780,6228,2607,6688,3239,1681,697,6134,271,595,2558,6974,5924,4090,3431,4299,2030,1894,1372,2761,2375,4346,4950,4106,2033,6677,6938,5611,5306,3976,3449,2415,3205,6447,5723,4370,1294,1744,3574,6965,1408,5939,828,4678,842,4043,2531,2778,1698,3460,1501,593,6802,5880,305,4961,5788,3990,963,703,1735,5879,344,618,446,1181,2525,6941,3315]\n",
    "inputs = []\n",
    "for i in random.sample(range(len(data_eval)), 25):\n",
    "    inputs.append((i, get_input(data_eval[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [(573,\n",
    "  'select t1.transcript_date, t1.transcript_id from transcripts as t1 join transcript_contents as t2 on t1.transcript_id = t2.transcript_id group by t1.transcript_id order by count(*) asc limit 1'),\n",
    " (653, 'select avg(employers) from poker_player'),\n",
    " (57,\n",
    "  'select distinct t1.fname from student as t1 join has_pet as t2 on t1.stuid = t2.stuid join pets as t3 on t2.petid = t3.petid where t3.pettype = \"cat\" or t3.pettype = \"dog\"'),\n",
    " (449, 'select count(*) from matches where year = 2013 or year = 2016'),\n",
    " (270,\n",
    "  'select manager_name, district from shop order by number_products desc limit 1'),\n",
    " (288,\n",
    "  'select count(*), t2.name from hiring as t1 join shop as t2 on t1.shop_id = t2.shop_id group by t1.shop_id'),\n",
    " (782,\n",
    "  'select code2 from country where language!= \"English\" and governmentform!= \"Republic\"'),\n",
    " (859,\n",
    "  'select count(*) from orchestra where major_record_format = \"CD\" or major_record_format = \"DVD\"'),\n",
    " (680, 'select name, birth_date from people order by name'),\n",
    " (541,\n",
    "  'select student_id, first_name, middle_name, last_name, count(*) from student_enrolment group by student_id order by count(*) desc limit 1'),\n",
    " (1003, 'select name from singer order by net_worth_millions asc'),\n",
    " (683,\n",
    "  'select name from people where people_id not in (select people_id from poker_player)'),\n",
    " (527, 'select section_name from sections order by section_name desc'),\n",
    " (38,\n",
    "  'select t2.name from concert as t1 join singer_in_concert as t2 on t1.singer_id = t2.singer_id where t1.year = 2014'),\n",
    " (766,\n",
    "  'select sum(population) from country where countrycode not in (select countrycode from countrylanguage where language = \"English\")'),\n",
    " (5,\n",
    "  'select avg(age), min(age), max(age) from singer where country = \"France\"'),\n",
    " (385, 'select age, hometown from teacher'),\n",
    " (613, 'select episode from tv_series order by rating'),\n",
    " (694,\n",
    "  'select t1.contestant_number, t1.contestant_name from contestants as t1 join votes as t2 on t1.contestant_number = t2.contestant_number group by t2.contestant_number having count(*) >= 2'),\n",
    " (466,\n",
    "  'select t1.winner_name from winner as t1 join matches as t2 on t1.winner_ioc = t2.winner_ioc join tourney_name as t3 on t2.tournament_name = t3.tournament_name where t3.turnier_name = \"Australian Open\" order by t1.winner_rank_points desc limit 1'),\n",
    " (430, 'select count(*) from players'),\n",
    " (940,\n",
    "  'select t1.owner_id, t1.zip_code from owners as t1 join dogs as t2 on t1.owner_id = t2.owner_id group by t1.owner_id order by sum(t2.charge_amount) desc limit 1'),\n",
    " (62,\n",
    "  'select major, age from student where stuid not in (select petid from pets where pettype = \"cat\")'),\n",
    " (98,\n",
    "  'select model from cars_data where weight  < (select avg(weight) from cars_data)'),\n",
    " (919,\n",
    "  'select state from owners union select t1.state from owners as t1 join professionals as t2 on t1.owner_id = t2.professional_id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for iv, sql in outputs:\n",
    "    thing = data_eval[iv]\n",
    "    result = await row_cosql_runner(thing, sql)\n",
    "    result |= {'row_number': iv, 'question': thing['question'], 'db_id': thing['db_id']}\n",
    "    results.append(result)\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df['query_is_equal'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import time\n",
    "\n",
    "# results = []\n",
    "# for i in random.sample(range(len(data_eval)), 400):\n",
    "#     thing = data_eval[i]\n",
    "#     print(i, thing['question'], thing['db_id'])\n",
    "#     result = await row_cosql_runner(thing)\n",
    "#     result |= {'row_number': i, 'question': thing['question'], 'db_id': thing['db_id']}\n",
    "#     results.append(result)\n",
    "#     df = pd.DataFrame(results)\n",
    "#     print(df['query_is_equal'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"unifiedskg_dataeval_25.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.gpt3_is_superset.sum()+(df.gpt3_is_same_set_of_rows.sum()-df.gpt3_is_equal.sum())) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"trial8.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('sketch': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "adcb619b33f588203bb576ea56ee3c20c6d0d8de6f18aca777331501d8e7497b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
