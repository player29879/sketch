{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857264b5-d685-42d5-8e0f-29af9399c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import bisect\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Plan is to build a data pipeline for wiki data tables\n",
    "# --> What tool should be used for this lineage? How to 'solve'\n",
    "#  -> the data-pipeline // lineage problem. \n",
    "# Step 1. Download the wiki files (assume completed atm)\n",
    "# Step 2. Unzip/load the index file\n",
    "# -> Extract the \"key\" space file (the perspective defining)\n",
    "# Step 3. \"turn the .bz2 text file data (per key)\"\n",
    "\n",
    "# oh, i want a fork in the DAG. maybe i should use dagster\n",
    "# ooh, i kinda want (or wish) that whatever notebook I write here\n",
    "# can get turned into an executable, monitorable, and reproducible pipeline.\n",
    "# Step 4. From each key, get the boolean of \"contains a table\"\n",
    "# Step 5. (this contains a table) requires a \"is a template file\" -> template-lookup -> template-execute\n",
    "#  -> Templates are also.. a DAG right, in theory? (can templates lookup other templates)\n",
    "#   this is something like a  .groupby(page_key).get('templates-referenced') and then .mapto(template) \n",
    "#   (apply the tempaltes to those keys)\n",
    "\n",
    "# -> Extract the \"text-block\" (with templates applied) for any document\n",
    "# and then keep the documents when they have a table in them (filter)\n",
    "\n",
    "# # From documents with tables on it, extract all of the tables into pandas dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountpath = '/media/share/DataSaves'\n",
    "indexpath = f\"{mountpath}/enwiki-20220101-pages-articles-multistream/enwiki-20220101-pages-articles-multistream-index.txt\"\n",
    "DUMP_FILE = f\"{mountpath}/enwiki-20220101-pages-articles-multistream/enwiki-20220101-pages-articles-multistream.xml.bz2\"\n",
    "OUTPUT_WRITING_PATH = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174be420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities (?)\n",
    "\n",
    "def get_byte_after(start_bytes, goal):\n",
    "    test = bisect.bisect_left(start_bytes, goal)\n",
    "    return start_bytes[test], start_bytes[test+1]\n",
    "\n",
    "def get_index_for_title(index_lookup, index_file, title):\n",
    "    try:\n",
    "        return index_lookup[title]\n",
    "    except:\n",
    "        for x in index_file:\n",
    "            if title in x:\n",
    "                return int(x.split(':')[0])\n",
    "\n",
    "def get_page_from_byte_offset(start, end):\n",
    "    raw = get_xml_pages(start, end)\n",
    "    soup = BeautifulSoup(raw, \"lxml\")\n",
    "    return soup.find_all(\"page\")\n",
    "    \n",
    "def get_page(index_lookup, start_bytes, title, index_file=None):\n",
    "    index = get_index_for_title(index_lookup, index_file, title)\n",
    "    pages = get_page_from_byte_offset(*get_byte_after(start_bytes, index))\n",
    "    print(len(pages))\n",
    "    for page in pages:\n",
    "        if title.lower() in page.title.string.lower():\n",
    "            return page\n",
    "    print(f\"couldn't find [{title}] in...\")\n",
    "    for i, page in enumerate(pages):\n",
    "        print(page.title.string)\n",
    "    print(\"=========== RETURNING NONE =================\")\n",
    "    return pages\n",
    "\n",
    "def get_xml_pages(start, end):\n",
    "    decomp = bz2.BZ2Decompressor()\n",
    "    with open(DUMP_FILE, 'rb') as f:\n",
    "        f.seek(start)\n",
    "        readback = f.read(end - start - 1)\n",
    "        page_xml = decomp.decompress(readback).decode()\n",
    "    return page_xml\n",
    "\n",
    "# page iterator\n",
    "def page_iterator(start_bytes, start=0, end=None):\n",
    "    for i in range(start, end or len(start_bytes)-1):\n",
    "        pages = get_page_from_byte_offset(start_bytes[i], start_bytes[i+1])\n",
    "        for page in pages:\n",
    "            yield page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c8265-6835-4856-bb55-0eff2109bb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6cc9e-d0ae-4303-99bf-2676349be06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(indexpath, \"r\") as f:\n",
    "    index_file = f.readlines()\n",
    "start_bytes = list(sorted(set([int(x.split(\":\")[0]) for x in index_file])))\n",
    "start_bytes.append(os.path.getsize(DUMP_FILE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77610352",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_bytes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1994fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_lookup = {}\n",
    "for i in index_file:\n",
    "    a, b, *title = i.split(':')\n",
    "    index_lookup[':'.join(title).rstrip()] = int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8532355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a \"document\" perspective feature, start_bytes index on 'wiki-page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f377e7-0de6-4535-9f2f-65604e850923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to find all the template files referenced, count them\n",
    "template_names = []\n",
    "for i in index_lookup:\n",
    "    if i.startswith('Template:'):\n",
    "        template_names.append(i)\n",
    "# omg, ~767802~ 731503 template!? ('template' is in i) vs. (i.startswith('Template:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_keys = set([x[9:] for x in template_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_indexes = [index_lookup[x] for x in template_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c605914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of blocks that have template pages: {len(set(template_indexes)) / len(start_bytes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80%, not much speedup to not just go through and full table-scan (seems like.. that's ~16 hours? from first glance)\n",
    "# 0.0 % 0.23284292221069336 EST 5059508.585977554 s\n",
    "# 0.04602123419745871 % 28.618114471435547 EST 62149.757650656386 s\n",
    "# 0.09204246839491742 % 58.4118857383728 EST 63400.3052304324 s\n",
    "# going to assume it's not worth writing the cost of the complexity of this algorithm (20% speedup)\n",
    "#  -> just go through and full table-scan (this is the O(N) loop)\n",
    "# hm. it's just a set-intersection though... sorta. (start_bytes & template_indexes) | (start_bytes & (template_indexes +1))\n",
    "# oh, but, that's enough for all the information, not enough for the \"spans\"\n",
    "# spans requires 2: (start_bytes & template_indexes), (start_bytes & (template_indexes +1))\n",
    "# maybe flatten them or something? and then add to the \"start_bytes\" iterator, if it sees a duplicate, assume this is a \"break\" and go to the next segment\n",
    "# keeps it monotonoically increasing, a \"fault-taulent\" (0->0 is okay request to subsystem), then the fallback is the the \"full set\"\n",
    "# so, like 0 1 2 .. 45 46, and then \"indexing\" into it with [0 0 5 5 9 12 12]  when parsed by neighbor (0, 0), (0, 5), (5, 9), (9, 12), (12, 12), covers the full \"set that contains\"\n",
    "# but then in a \"coded\" style parser, (0, 0) include row 0, then skip to (5, 5), just 5, then skip to include (9, 12)\n",
    "# --> I wonder what the fastest version of this algorithm is (like, some sorta subset identifiying label using \"monotonically increasing\" integer of spans)\n",
    "# i guess like, at _worst case_, you have the noisy subset (eg. 50% of them are included, in the peak of the 2^N of subset space)\n",
    "# -- in that case, the \"worst-case\" solution, just [0 0 1 1 2 2 3 3 4 4 5 5 ...] .. is used. \n",
    "#   but in best case, [55 56] (single index, for instance), its pretty cheap to give the span\n",
    "# --> This is a distraction --- I need to go back to original problem (maybe this is why i said (first) but didn't listen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bda21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL SCAN OPERATION, LIKELY TO BE EXPENSIVE\n",
    "\n",
    "# estimated_total = len(start_bytes)*100\n",
    "# st = time.time()\n",
    "# template_pages = {}\n",
    "# for i, page in enumerate(page_iterator(start_bytes)):\n",
    "#     if page.title.text.startswith('Template:'):\n",
    "#         template_pages[page.title.text] = page\n",
    "#         print(\"Found a template page\")\n",
    "#         if len(template_pages) > 1000:\n",
    "#             print(\"we're up to 1000\")\n",
    "#             break\n",
    "#     if i%10000 == 0:\n",
    "#         # estimated time remaining:\n",
    "#         print(i*100 / estimated_total, '%', time.time() - st, 'EST', (time.time() - st) * (estimated_total - i) / (i+1), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for match in re.finditer(r'\\{\\{(.*)?\\}\\}', template_pages['Template:Periodic table'].text):\n",
    "#     template_internals = match.groups()[0]\n",
    "#     if template_internals in template_keys:\n",
    "#         print(template_internals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83094420",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(start_bytes)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r'\\{\\{(.*)?\\}\\}' -> get tuples of all matches for \"expressions\"\n",
    "# r'(.*)' -> matches anything\n",
    "\n",
    "def get_matches(start_bytes, expression, start=0, end=None, pagelimit=None, limit=None):\n",
    "    c = 0\n",
    "    for i, page in enumerate(page_iterator(start_bytes, start=start, end=end)):\n",
    "        # extract all '{{.*}}' sections from page.text using regex\n",
    "        for j, match in enumerate(re.finditer(expression, page.text)):\n",
    "            template_internals = match.groups()[0]\n",
    "            c += 1\n",
    "            yield (page, page.title.text, j, template_internals) # page may not be necessary\n",
    "        if pagelimit:\n",
    "            if i > pagelimit:\n",
    "                break\n",
    "        if limit:\n",
    "            if c > limit:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleofdc = set()\n",
    "for anydoublecurly in get_matches(start_bytes, r'\\{\\{(.*)?\\}\\}', start=0, end=None):\n",
    "    exampleofdc.add(anydoublecurly)\n",
    "    if len(exampleofdc) > 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a917d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in exampleofdc:\n",
    "    if 'table' in x:\n",
    "        print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82280e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  '{| ' to ' |}'\n",
    "\n",
    "trial = set()\n",
    "for anydoublecurly in get_matches(start_bytes, r'\\{\\|(.*)?\\|\\}', start=0, end=None, limit=100000):\n",
    "    trial.add(anydoublecurly)\n",
    "    if len(trial) > 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  class=\"wikitable\n",
    "\n",
    "trial = set()\n",
    "for anydoublecurly in get_matches(start_bytes, r'(class=\"wikitable)', start=0, end=None, limit=100000):\n",
    "    trial.add(anydoublecurly)\n",
    "    if len(trial) > 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "page, *_ = trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page, *meta= page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tables(page):\n",
    "#     # class=\"wikitable)\n",
    "#     for match in re.finditer(r'\\{\\|([\\S\\s]*?)\\|\\}', page.text):\n",
    "#         if 'table' in match.groups()[0]:\n",
    "#             yield match.groups()[0]\n",
    "\n",
    "# # print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65401e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "goal_path = os.path.join(OUTPUT_WRITING_PATH, 'wikitable_extract_220814_test')\n",
    "os.makedirs(goal_path, exist_ok=True)\n",
    "block_of_writing = []\n",
    "for i, (page, page_title_text, j, template_internals) in enumerate(get_matches(start_bytes, r'\\{\\|([\\S\\s]*?)\\|\\}', start=0, end=None, limit=100000)):\n",
    "    if 'wikitable' in template_internals:\n",
    "        block_of_writing.append((page_title_text, j, template_internals, page.text))\n",
    "        if len(block_of_writing) > 1000:\n",
    "            print(\"Writing block\")\n",
    "            pd.DataFrame(block_of_writing, columns=('page_title_text', 'j', 'template_internals', 'page_text')).to_parquet(os.path.join(goal_path, f'{i}.parquet'), engine='pyarrow', compression='gzip')\n",
    "            block_of_writing = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./wikitable_extract_220814_test/*\n",
    "!ls -lh ./wikitable_extract_220814_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a25a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fac509",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f668aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "'|' in page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikidata is a graph database (which is not what I'm looking at right now)\n",
    "# I wonder if there's some... English <-> Sparkle writer that could \"enumerate\" a lot of the useful \n",
    "# language things, and also on search, attempt to modify from them for new ones. \n",
    "#-> i just want to be able to query wikidata with text i guess is what im saying\n",
    "#  unlock the power of the language, and unlock the power of the world  # I laughed at Co-pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea865fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_with_tables = []\n",
    "for i, page in enumerate(page_iterator(start_bytes)):\n",
    "    if 'wikitable' in page.text:\n",
    "        pages_with_tables.append(page)\n",
    "        print(f\"{i} {page.title.string}\")\n",
    "        if len(pages_with_tables) > 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ad641",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_with_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc15d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = get_page(index_lookup, start_bytes, template_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aae107",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_page(index_lookup, start_bytes, template_names[1211]).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5efaa3-6436-446a-981d-7289d5cbe1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f4306-8599-4d9e-8a37-ed5dbfd2b205",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2af1c4-24b4-4106-ae51-6c9b8bd937f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_byte_after(13130921384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f93477-84a3-4f4f-9e3d-bf932413186c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe2e56-386f-4843-ba71-4eb8fa644d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2537c1-24d7-40e9-9f5c-77c4031f6bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8915b-b153-4799-8454-22fc0ba5b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_page('NBA player statistics start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddd6df-a048-4a0e-9826-c1070953095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c339ee-1922-4e84-bd8e-f4ab8690ab1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0b82f-3e4b-4f97-836e-1ae5a543c4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878936a1-3e12-4f12-8303-4bdc6417290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_html('https://en.wikipedia.org/wiki/List_of_Daredevil_(TV_series)_characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ca54d-06bb-450f-a0f4-bb11d51a7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4349a-b55e-477d-9a8f-6a3eafd59908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('sketch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "adcb619b33f588203bb576ea56ee3c20c6d0d8de6f18aca777331501d8e7497b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
