{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "# download_file_from_google_drive(\"1m_M2VOjM7Xxq0z9hxgGdYIGIKhxOIU-F\", \"spider_train.json\")\n",
    "# download_file_from_google_drive(\"1twy32bdOYcTY8HXrISm1vTBMMjTzI9Fa\", \"spider_eval.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.load(open(\"spider_train.json\"))\n",
    "for i, row in enumerate(data):\n",
    "    row['row_id'] = f\"spider_train|{i}\"\n",
    "\n",
    "data_eval = json.load(open(\"spider_eval.json\"))\n",
    "for i, row in enumerate(data):\n",
    "    row['row_id'] = f\"spider_eval|{i}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sketch.examples.prompt_machine import *\n",
    "PM_SETTINGS[\"VERBOSE\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_input = json.load(open(\"spider/train_spider.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_KEY = # GET THIS FROM SOMEONE.\n",
    "\n",
    "# def get_gpt3_response(prompt):\n",
    "#     headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "#     data = {\"prompt\": prompt, \"max_tokens\": 200, \"temperature\": 0, \"model\": \"text-davinci-002\"}\n",
    "#     response = requests.post(\"https://api.openai.com/v1/completions\", headers=headers, json=data)\n",
    "#     return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_prompt(row, answer=None):\n",
    "    return f\"Question: {row['text_in']};\\nSchema Information: {row['struct_in']};\\nSQL:\\n```\\n\" + (f\"{answer}\\n```\\n\" if answer else \"\")\n",
    "\n",
    "def get_preprompt(ids):\n",
    "    return '\\n'.join([line_prompt(data[id], data[id]['query'])+\"\\n\" for id in ids])\n",
    "\n",
    "preprompt123 = get_preprompt([123, 456, 1516, 1522])\n",
    "def get_prompt(row):\n",
    "    return preprompt123 + line_prompt(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPIDER_DB_PATH = \"/home/jawaugh/benchmarks/raw/spider/spider/database/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare executed results\n",
    "import os\n",
    "\n",
    "def get_result(db_name, sql):\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect(os.path.join(SPIDER_DB_PATH, f\"{db_name}/{db_name}.sqlite\"))\n",
    "    conn.text_factory = lambda b: b.decode(errors=\"ignore\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    return cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3responsder = GPT3Prompt(\"standard_prompt\", \"{{prompt}}\", temperature=0, model_name=\"code-davinci-002\", stop=\"```\")\n",
    "\n",
    "def run_a_row(row, verbose=True):\n",
    "    prompt = get_prompt(row)\n",
    "\n",
    "    # print(\"PROMPT USED FOR GPT3\")\n",
    "    # print(prompt)\n",
    "    # print(\"-------------------\")\n",
    "\n",
    "    try:\n",
    "        gpt3_answer = gpt3responsder(prompt)\n",
    "        # gpt3_answer = \"wow, this is a great answer\"\n",
    "    except:\n",
    "        gpt3_answer = \"Something failed in querying openai\"\n",
    "    query_answer = row['query']\n",
    "    seq_out_answer = row['seq_out']\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Question: \", row['question'])\n",
    "        print(\"GOLD: \", query_answer)\n",
    "        print(\"GPT3: \", gpt3_answer)\n",
    "        print(\"SEQ_OUT: \", seq_out_answer)\n",
    "\n",
    "    try:\n",
    "        real_data = get_result(row['db_id'], query_answer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        real_data = \"EXECUTION ERROR\"\n",
    "    # attempt to run all 3 and get data\n",
    "    try:\n",
    "        gpt3_data = get_result(row['db_id'], gpt3_answer)\n",
    "    except:\n",
    "        gpt3_data = \"EXECUTION ERROR\"\n",
    "    try:\n",
    "        seq_out_data = get_result(row['db_id'], seq_out_answer)\n",
    "    except:\n",
    "        seq_out_data = \"EXECUTION ERROR\"\n",
    "    if verbose:\n",
    "        print(\"REAL: \", real_data)\n",
    "        print(\"GPT3: \", gpt3_data)\n",
    "        print(\"T5: \", seq_out_data)\n",
    "    sql = {'query_answer': query_answer, 'gpt3_answer': gpt3_answer, 'seq_out_answer': seq_out_answer}\n",
    "    return sql, (real_data, gpt3_data, seq_out_data)\n",
    "\n",
    "def get_scores(real_data, gpt3_data, seq_out_data):\n",
    "    scores = {}\n",
    "    scores['gpt3_correct'] = gpt3_data == real_data\n",
    "    scores['seq_out_correct'] = seq_out_data == real_data\n",
    "    scores['gpt3_executed'] = gpt3_data != \"EXECUTION ERROR\"\n",
    "    scores['seq_out_executed'] = seq_out_data != \"EXECUTION ERROR\"\n",
    "    scores['query_executed'] = real_data != \"EXECUTION ERROR\"\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_result_run(dataset):\n",
    "    all_results = []\n",
    "    print(f\"{len(dataset)}\")\n",
    "    for i, row in enumerate(dataset):\n",
    "        time.sleep(10.0)  # add a sleep to prevent any rate limits for now\n",
    "        sql, results = run_a_row(row, verbose=False)\n",
    "        scores = get_scores(*results)\n",
    "        all_results.append({'id': row['row_id'], **sql, **scores})\n",
    "        print(f\"{i} done\")\n",
    "        temp_results = pd.DataFrame(all_results)\n",
    "        print(\"Ran for {} rows\".format(len(temp_results)))\n",
    "        print(temp_results.seq_out_correct.mean()*100, \"% of seq_out_correct\")\n",
    "        print(temp_results.seq_out_executed.mean()*100, \"% of seq_out_executed\")\n",
    "        print(temp_results.gpt3_correct.mean()*100, \"% of gpt3_correct\")\n",
    "        print(temp_results.gpt3_executed.mean()*100, \"% of gpt3_executed\")\n",
    "        print(temp_results.query_executed.mean()*100, \"% of query_executed\")\n",
    "        print(\"==\"*40)\n",
    "    return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_result_run(random.sample(data, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ran for {} rows\".format(len(df)))\n",
    "print(df.seq_out_correct.mean()*100, \"% of seq_out_correct\")\n",
    "print(df.seq_out_executed.mean()*100, \"% of seq_out_executed\")\n",
    "print(df.gpt3_correct.mean()*100, \"% of gpt3_correct\")\n",
    "print(df.gpt3_executed.mean()*100, \"% of gpt3_executed\")\n",
    "print(df.query_executed.mean()*100, \"% of query_executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.rename(columns={'t5_answer': 'seq_out_answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query_answer', 'GPT3_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"gpt3_results_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.gpt3_answer == 'COMPLETION_ERROR').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    thing = df[df['gpt3_correct'] == False].sample()\n",
    "    failed_result = thing.reset_index().iloc[0].to_dict()\n",
    "    failed_row = data[int(failed_result['id'].split('|')[1])]\n",
    "    print(\"Question: \", failed_row['question'])\n",
    "\n",
    "    print(\"GPT3 Query:\", failed_result['gpt3_answer'])\n",
    "    print(\"GOLD Query:\", failed_result['query_answer'])\n",
    "\n",
    "    try:\n",
    "        r1 = get_result(failed_row['db_id'], failed_result['gpt3_answer'])\n",
    "        if len(r1) > 10:\n",
    "            r1 = r1[:10]\n",
    "            print(\"Truncating GPT3 response\")\n",
    "        print(\"GPT3 QueryResult:\", r1)\n",
    "    except Exception as e:\n",
    "        print(\"GPT3 QueryFAILED:\", e)\n",
    "    r2 = get_result(failed_row['db_id'], failed_result['query_answer'])\n",
    "    if len(r2) > 10:\n",
    "        r2 = r2[:10]\n",
    "        print(\"Truncating GOLD response\")\n",
    "    print(\"GOLD QueryResult:\", r2)\n",
    "    print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1549]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = get_gpt3_response(get_prompt(data[555]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"GPT3_vs_T5.parquet\")['gpt3_correct'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "31/44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[555]['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[555]['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('sketch': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "adcb619b33f588203bb576ea56ee3c20c6d0d8de6f18aca777331501d8e7497b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
